{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***`Understand NLP`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reading the files from current location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [file_name.split(\"\\\\\")[-1] for file_name in glob.glob(os.getcwd()+'\\\\doc*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1.txt', 'doc2.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the corpus from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(root=os.getcwd(),fileids=file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1.txt', 'doc2.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Courpus Paras`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.'], ['\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.'], ['\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.'], ['\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"']], [['\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.'], ['\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.'], ['But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.'], ['\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.'], ['\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']]]\n"
     ]
    }
   ],
   "source": [
    "print([para for para in corpus.paras()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Courpus Sentences`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.'], ['\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.'], ['\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.'], ['\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"'], ['\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.'], ['\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.'], ['But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.'], ['\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.'], ['\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']]\n"
     ]
    }
   ],
   "source": [
    "corpus_sents = [sent for sent in corpus.sents()]\n",
    "print(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Courpus Words`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"', '\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.', 'But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']\n"
     ]
    }
   ],
   "source": [
    "corpus_words = [word for word in corpus.words()]\n",
    "print(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`English Stopwords`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['not','nor','no']:\n",
    "    eng_stopwords.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Cleaning the Corpus`***\n",
    "- ##### **Removing special characters**\n",
    "- ##### **Removing unwanted spaces**\n",
    "- ##### **Lower case the words**\n",
    "- ##### **Tokenizing the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.'], ['\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.'], ['\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.'], ['\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"'], ['\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.'], ['\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.'], ['But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.'], ['\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.'], ['\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my name is rajesh sharma'], ['i love working on data science projects'], ['the nexon car is very affordable'], ['the pizza was cheap tasty and delicious'], ['the dominoz pizza is tasty and loaded'], ['my name is raman revti sharma'], ['i love doing data analytics'], ['the tata nexon car is very stylish dynamic and has a strong build'], ['but their after sales service is not good'], ['the pizza in the party was tasty and cheesy'], ['the dominoz tacco is always cripy and fingerlicious']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_sent = []\n",
    "for sent in corpus_sents:\n",
    "    process_sent = [re.sub('[^A-Za-z]+', ' ', str(sent)).strip().lower()]\n",
    "    cleaned_sent.append(process_sent)\n",
    "    \n",
    "print(cleaned_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Storing text messages in a DataFrame`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message\n",
       "0    0                                           my name is rajesh sharma\n",
       "1    1                            i love working on data science projects\n",
       "2    2                                   the nexon car is very affordable\n",
       "3    3                            the pizza was cheap tasty and delicious\n",
       "4    4                              the dominoz pizza is tasty and loaded\n",
       "5    5                                      my name is raman revti sharma\n",
       "6    6                                        i love doing data analytics\n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build\n",
       "8    8                          but their after sales service is not good\n",
       "9    9                        the pizza in the party was tasty and cheesy\n",
       "10  10                the dominoz tacco is always cripy and fingerlicious"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame(cleaned_sent).reset_index()\n",
    "text_df.columns = ['Id','Pre_processed_Message']\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Removing Stopwords and Tokenization`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \n",
       "0                                [name, rajesh, sharma]  \n",
       "1              [love, working, data, science, projects]  \n",
       "2                              [nexon, car, affordable]  \n",
       "3                      [pizza, cheap, tasty, delicious]  \n",
       "4                       [dominoz, pizza, tasty, loaded]  \n",
       "5                          [name, raman, revti, sharma]  \n",
       "6                               [love, data, analytics]  \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]  \n",
       "8                           [sales, service, not, good]  \n",
       "9                         [pizza, party, tasty, cheesy]  \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Tokens'] = text_df['Pre_processed_Message'].apply(lambda row: [word for word in row.split(\" \") if word not in eng_stopwords])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Stemming`***\n",
    "#### ***`Porter Stemmer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter_Stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                                [name, rajesh, sharma]   \n",
       "1              [love, working, data, science, projects]   \n",
       "2                              [nexon, car, affordable]   \n",
       "3                      [pizza, cheap, tasty, delicious]   \n",
       "4                       [dominoz, pizza, tasty, loaded]   \n",
       "5                          [name, raman, revti, sharma]   \n",
       "6                               [love, data, analytics]   \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]   \n",
       "8                           [sales, service, not, good]   \n",
       "9                         [pizza, party, tasty, cheesy]   \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]   \n",
       "\n",
       "                                         Porter_Stems  \n",
       "0                              [name, rajesh, sharma]  \n",
       "1                 [love, work, data, scienc, project]  \n",
       "2                                [nexon, car, afford]  \n",
       "3                       [pizza, cheap, tasti, delici]  \n",
       "4                       [dominoz, pizza, tasti, load]  \n",
       "5                        [name, raman, revti, sharma]  \n",
       "6                                [love, data, analyt]  \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]  \n",
       "8                           [sale, servic, not, good]  \n",
       "9                       [pizza, parti, tasti, cheesi]  \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Porter_Stems'] = text_df['Tokens'].apply(lambda row: [port_stem.stem(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Porter Stemmer is simply chopping off the tails of the words. Not a good way!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Snowball Stemmer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stem = SnowballStemmer(language='english',ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter_Stems</th>\n",
       "      <th>Snowball_Stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                                [name, rajesh, sharma]   \n",
       "1              [love, working, data, science, projects]   \n",
       "2                              [nexon, car, affordable]   \n",
       "3                      [pizza, cheap, tasty, delicious]   \n",
       "4                       [dominoz, pizza, tasty, loaded]   \n",
       "5                          [name, raman, revti, sharma]   \n",
       "6                               [love, data, analytics]   \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]   \n",
       "8                           [sales, service, not, good]   \n",
       "9                         [pizza, party, tasty, cheesy]   \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]   \n",
       "\n",
       "                                         Porter_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                       Snowball_Stems  \n",
       "0                              [name, rajesh, sharma]  \n",
       "1                 [love, work, data, scienc, project]  \n",
       "2                                [nexon, car, afford]  \n",
       "3                       [pizza, cheap, tasti, delici]  \n",
       "4                       [dominoz, pizza, tasti, load]  \n",
       "5                        [name, raman, revti, sharma]  \n",
       "6                                [love, data, analyt]  \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]  \n",
       "8                           [sale, servic, not, good]  \n",
       "9                       [pizza, parti, tasti, cheesi]  \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Snowball_Stems'] = text_df['Tokens'].apply(lambda row: [snow_stem.stem(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Snowball Stemmer is considered as a strongest method for taking a word to its root form. However, here, I didn't oberserved any difference with Porter Stemmer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Lancaster Stemmer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanc_stem = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter_Stems</th>\n",
       "      <th>Snowball_Stems</th>\n",
       "      <th>Lancaster_Stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[nam, rajesh, sharm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[lov, work, dat, sci, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizz, cheap, tasty, delicy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizz, tasty, load]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[nam, ram, revt, sharm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[lov, dat, analys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tat, nexon, car, styl, dynam, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sal, serv, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizz, party, tasty, cheesy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripy, fingerlicy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                                [name, rajesh, sharma]   \n",
       "1              [love, working, data, science, projects]   \n",
       "2                              [nexon, car, affordable]   \n",
       "3                      [pizza, cheap, tasty, delicious]   \n",
       "4                       [dominoz, pizza, tasty, loaded]   \n",
       "5                          [name, raman, revti, sharma]   \n",
       "6                               [love, data, analytics]   \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]   \n",
       "8                           [sales, service, not, good]   \n",
       "9                         [pizza, party, tasty, cheesy]   \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]   \n",
       "\n",
       "                                         Porter_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                       Snowball_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                  Lancaster_Stems  \n",
       "0                            [nam, rajesh, sharm]  \n",
       "1                  [lov, work, dat, sci, project]  \n",
       "2                            [nexon, car, afford]  \n",
       "3                    [pizz, cheap, tasty, delicy]  \n",
       "4                    [dominoz, pizz, tasty, load]  \n",
       "5                         [nam, ram, revt, sharm]  \n",
       "6                              [lov, dat, analys]  \n",
       "7   [tat, nexon, car, styl, dynam, strong, build]  \n",
       "8                          [sal, serv, not, good]  \n",
       "9                    [pizz, party, tasty, cheesy]  \n",
       "10     [dominoz, tacco, alway, cripy, fingerlicy]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Lancaster_Stems'] = text_df['Tokens'].apply(lambda row: [lanc_stem.stem(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitely Lancaster Stemmer is not a good approach to bring a word to its root form. It is chopping-off the vowels from the tails of the words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Lemmatizers`***\n",
    "##### **Reference Links**\n",
    "\n",
    "- https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \n",
    "- https://www.nltk.org/book/ch05.html\n",
    "\n",
    "\n",
    "#### ***`Wordnet Lemmatizer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnet_lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Trying to understand the working of Wordnet Lemmatizer on some examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word along its Stems(Snowball) and Lemmas(Wordnet):\n",
      "\n",
      "'cows' ---> 'cow' ---> 'cow'\n",
      "'cow' ---> 'cow' ---> 'cow'\n",
      "'boys' ---> 'boy' ---> 'boy'\n",
      "'males' ---> 'male' ---> 'male'\n",
      "'females' ---> 'femal' ---> 'female'\n",
      "'accesses' ---> 'access' ---> 'access'\n",
      "'accessed' ---> 'access' ---> 'accessed'\n",
      "'caring' ---> 'care' ---> 'caring'\n",
      "'cares' ---> 'care' ---> 'care'\n",
      "'watches' ---> 'watch' ---> 'watch'\n",
      "'watched' ---> 'watch' ---> 'watched'\n",
      "'watching' ---> 'watch' ---> 'watching'\n",
      "'is' ---> 'is' ---> 'is'\n",
      "'are' ---> 'are' ---> 'are'\n",
      "'were' ---> 'were' ---> 'were'\n",
      "'we' ---> 'we' ---> 'we'\n",
      "'did' ---> 'did' ---> 'did'\n",
      "'does' ---> 'does' ---> 'doe'\n",
      "'fruits' ---> 'fruit' ---> 'fruit'\n",
      "'fruity' ---> 'fruiti' ---> 'fruity'\n",
      "'tastes' ---> 'tast' ---> 'taste'\n",
      "'tasty' ---> 'tasti' ---> 'tasty'\n",
      "'beauties' ---> 'beauti' ---> 'beauty'\n",
      "'beautification' ---> 'beautif' ---> 'beautification'\n",
      "'beauty' ---> 'beauti' ---> 'beauty'\n",
      "'beautiful' ---> 'beauti' ---> 'beautiful'\n",
      "'unreliable' ---> 'unreli' ---> 'unreliable'\n",
      "'unreliability' ---> 'unreli' ---> 'unreliability'\n",
      "'explain' ---> 'explain' ---> 'explain'\n",
      "'explaines' ---> 'explain' ---> 'explaines'\n",
      "'explanation' ---> 'explan' ---> 'explanation'\n"
     ]
    }
   ],
   "source": [
    "example_words = ['cows','cow','boys','males','females','accesses','accessed','caring','cares','watches','watched','watching','is','are',\\\n",
    "                 'were','we','did','does','fruits','fruity','tastes','tasty','beauties','beautification','beauty','beautiful','unreliable',\\\n",
    "                 'unreliability','explain','explaines','explanation']\n",
    "\n",
    "print(\"Given word along its Stems(Snowball) and Lemmas(Wordnet):\\n\")\n",
    "for word in example_words:\n",
    "    print(\"'{}' ---> '{}' ---> '{}'\".format(word,snow_stem.stem(word),wnet_lemma.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Here, I have observed that Lemmatizer works good only in the case of plural words that belongs to the category of 'Inflectional Bound Morphemes'. It doesn't really work with other kind or forms of words. On the other hand, Stemmers firmly believe on chopping the tails of words which many times leads to incorrect words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = (\"Waaooo, such an amazing match. I really enjoyed watching this event of WWE and pro wrestling. \\\n",
    "And, they are my stress busters too.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waaooo,',\n",
       " 'such',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'match',\n",
       " 'I',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'watching',\n",
       " 'this',\n",
       " 'event',\n",
       " 'of',\n",
       " 'WWE',\n",
       " 'and',\n",
       " 'pro',\n",
       " 'wrestling',\n",
       " 'And,',\n",
       " 'they',\n",
       " 'are',\n",
       " 'my',\n",
       " 'stress',\n",
       " 'buster',\n",
       " 'too']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnet_lemma.lemmatize(word.replace(\".\",\"\")) for word in test_sentence.split(\" \")]      ## Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nothing really changed here other than busters got converted to buster.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waaooo,',\n",
       " 'such',\n",
       " 'an',\n",
       " 'amaz',\n",
       " 'match',\n",
       " 'i',\n",
       " 'realli',\n",
       " 'enjoy',\n",
       " 'watch',\n",
       " 'this',\n",
       " 'event',\n",
       " 'of',\n",
       " 'wwe',\n",
       " 'and',\n",
       " 'pro',\n",
       " 'wrestl',\n",
       " 'and,',\n",
       " 'they',\n",
       " 'are',\n",
       " 'my',\n",
       " 'stress',\n",
       " 'buster',\n",
       " 'too']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[snow_stem.stem(word.replace(\".\",\"\")) for word in test_sentence.split(\" \")]      ## Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Good amount of words tail chopping have been performed here and it looks like Stemmer lowercase the words before performing any action.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Wordnet Lemmatizer with POS(Part-of-speech) tagging`***\n",
    "\n",
    "##### **CASE-I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('watching',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('watching',pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('watching',pos='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('watching',pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('watching',pos='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In the previous and above examples lemmatizer didn't performed really well. Earlier 'watching' was not converted to 'watch' but here it got converted to 'watch' when we used the POS tag as 'verb'. This is beacuse we provided the correct part-of-speech tag (POS tag) as the second argument to lemmatize(). Sometimes, the same word can have a multiple lemmas based on the meaning / context.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **CASE-II :: POS-TAG as VERB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word with its POS tag along, Stems(Snowball) and Lemmas(Wordnet):\n",
      "\n",
      "[('cows', 'NNS')] --> 'cow' ---> 'cow'\n",
      "[('cow', 'NN')] --> 'cow' ---> 'cow'\n",
      "[('boys', 'NNS')] --> 'boy' ---> 'boys'\n",
      "[('males', 'NNS')] --> 'male' ---> 'males'\n",
      "[('females', 'NNS')] --> 'femal' ---> 'females'\n",
      "[('accesses', 'NNS')] --> 'access' ---> 'access'\n",
      "[('accessed', 'VBN')] --> 'access' ---> 'access'\n",
      "[('caring', 'VBG')] --> 'care' ---> 'care'\n",
      "[('cares', 'NNS')] --> 'care' ---> 'care'\n",
      "[('watches', 'NNS')] --> 'watch' ---> 'watch'\n",
      "[('watched', 'VBN')] --> 'watch' ---> 'watch'\n",
      "[('watching', 'VBG')] --> 'watch' ---> 'watch'\n",
      "[('is', 'VBZ')] --> 'is' ---> 'be'\n",
      "[('are', 'VBP')] --> 'are' ---> 'be'\n",
      "[('were', 'VBD')] --> 'were' ---> 'be'\n",
      "[('we', 'PRP')] --> 'we' ---> 'we'\n",
      "[('did', 'VBD')] --> 'did' ---> 'do'\n",
      "[('does', 'VBZ')] --> 'does' ---> 'do'\n",
      "[('fruits', 'NNS')] --> 'fruit' ---> 'fruit'\n",
      "[('fruity', 'NN')] --> 'fruiti' ---> 'fruity'\n",
      "[('tastes', 'NNS')] --> 'tast' ---> 'taste'\n",
      "[('tasty', 'NN')] --> 'tasti' ---> 'tasty'\n",
      "[('beauties', 'NNS')] --> 'beauti' ---> 'beauties'\n",
      "[('beautes', 'NNS')] --> 'beaut' ---> 'beautes'\n",
      "[('beautification', 'NN')] --> 'beautif' ---> 'beautification'\n",
      "[('beauty', 'NN')] --> 'beauti' ---> 'beauty'\n",
      "[('beautiful', 'NN')] --> 'beauti' ---> 'beautiful'\n",
      "[('unreliable', 'JJ')] --> 'unreli' ---> 'unreliable'\n",
      "[('unreliability', 'NN')] --> 'unreli' ---> 'unreliability'\n",
      "[('explain', 'NN')] --> 'explain' ---> 'explain'\n",
      "[('explaines', 'NNS')] --> 'explain' ---> 'explain'\n",
      "[('explanation', 'NN')] --> 'explan' ---> 'explanation'\n",
      "[('refuse', 'NN')] --> 'refus' ---> 'refuse'\n",
      "[('deny', 'NN')] --> 'deni' ---> 'deny'\n",
      "[('good', 'JJ')] --> 'good' ---> 'good'\n",
      "[('better', 'RBR')] --> 'better' ---> 'better'\n",
      "[('best', 'JJS')] --> 'best' ---> 'best'\n"
     ]
    }
   ],
   "source": [
    "example_words = ['cows','cow','boys','males','females','accesses','accessed','caring','cares','watches','watched','watching','is','are',\\\n",
    "                 'were','we','did','does','fruits','fruity','tastes','tasty','beauties','beautes','beautification','beauty','beautiful',\\\n",
    "                 'unreliable','unreliability','explain','explaines','explanation','refuse','deny','good','better','best']\n",
    "\n",
    "print(\"Given word with its POS tag along, Stems(Snowball) and Lemmas(Wordnet):\\n\")\n",
    "for word in example_words:\n",
    "    print(\"{} --> '{}' ---> '{}'\".format(nltk.pos_tag([word]),snow_stem.stem(word),wnet_lemma.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **CASE-II :: POS-TAG as ADVERB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word with its POS tag along, Stems(Snowball) and Lemmas(Wordnet):\n",
      "\n",
      "[('cows', 'NNS')] --> 'cow' ---> 'cows'\n",
      "[('cow', 'NN')] --> 'cow' ---> 'cow'\n",
      "[('boys', 'NNS')] --> 'boy' ---> 'boys'\n",
      "[('males', 'NNS')] --> 'male' ---> 'males'\n",
      "[('females', 'NNS')] --> 'femal' ---> 'females'\n",
      "[('accesses', 'NNS')] --> 'access' ---> 'accesses'\n",
      "[('accessed', 'VBN')] --> 'access' ---> 'accessed'\n",
      "[('caring', 'VBG')] --> 'care' ---> 'caring'\n",
      "[('cares', 'NNS')] --> 'care' ---> 'cares'\n",
      "[('watches', 'NNS')] --> 'watch' ---> 'watches'\n",
      "[('watched', 'VBN')] --> 'watch' ---> 'watched'\n",
      "[('watching', 'VBG')] --> 'watch' ---> 'watching'\n",
      "[('is', 'VBZ')] --> 'is' ---> 'is'\n",
      "[('are', 'VBP')] --> 'are' ---> 'are'\n",
      "[('were', 'VBD')] --> 'were' ---> 'were'\n",
      "[('we', 'PRP')] --> 'we' ---> 'we'\n",
      "[('did', 'VBD')] --> 'did' ---> 'did'\n",
      "[('does', 'VBZ')] --> 'does' ---> 'does'\n",
      "[('fruits', 'NNS')] --> 'fruit' ---> 'fruits'\n",
      "[('fruity', 'NN')] --> 'fruiti' ---> 'fruity'\n",
      "[('tastes', 'NNS')] --> 'tast' ---> 'tastes'\n",
      "[('tasty', 'NN')] --> 'tasti' ---> 'tasty'\n",
      "[('beauties', 'NNS')] --> 'beauti' ---> 'beauties'\n",
      "[('beautes', 'NNS')] --> 'beaut' ---> 'beautes'\n",
      "[('beautification', 'NN')] --> 'beautif' ---> 'beautification'\n",
      "[('beauty', 'NN')] --> 'beauti' ---> 'beauty'\n",
      "[('beautiful', 'NN')] --> 'beauti' ---> 'beautiful'\n",
      "[('unreliable', 'JJ')] --> 'unreli' ---> 'unreliable'\n",
      "[('unreliability', 'NN')] --> 'unreli' ---> 'unreliability'\n",
      "[('explain', 'NN')] --> 'explain' ---> 'explain'\n",
      "[('explaines', 'NNS')] --> 'explain' ---> 'explaines'\n",
      "[('explanation', 'NN')] --> 'explan' ---> 'explanation'\n",
      "[('refuse', 'NN')] --> 'refus' ---> 'refuse'\n",
      "[('deny', 'NN')] --> 'deni' ---> 'deny'\n",
      "[('good', 'JJ')] --> 'good' ---> 'good'\n",
      "[('better', 'RBR')] --> 'better' ---> 'well'\n",
      "[('best', 'JJS')] --> 'best' ---> 'best'\n"
     ]
    }
   ],
   "source": [
    "example_words = ['cows','cow','boys','males','females','accesses','accessed','caring','cares','watches','watched','watching','is','are',\\\n",
    "                 'were','we','did','does','fruits','fruity','tastes','tasty','beauties','beautes','beautification','beauty','beautiful',\\\n",
    "                 'unreliable','unreliability','explain','explaines','explanation','refuse','deny','good','better','best']\n",
    "\n",
    "print(\"Given word with its POS tag along, Stems(Snowball) and Lemmas(Wordnet):\\n\")\n",
    "for word in example_words:\n",
    "    print(\"{} --> '{}' ---> '{}'\".format(nltk.pos_tag([word]),snow_stem.stem(word),wnet_lemma.lemmatize(word,pos='r')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clearly, POS TAG plays a crucial role in understanding the meaning or context of the sentence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Check the TAG type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RBR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This way we see some help on TAG codes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **CASE-III :: POS-TAG of words in a sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Waaooo', 'NNP'),\n",
       " ('such', 'PDT'),\n",
       " ('an', 'DT'),\n",
       " ('amazing', 'JJ'),\n",
       " ('match', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('really', 'RB'),\n",
       " ('enjoyed', 'VBD'),\n",
       " ('watching', 'VBG'),\n",
       " ('this', 'DT'),\n",
       " ('event', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('WWE', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('pro', 'JJ'),\n",
       " ('wrestling', 'NN'),\n",
       " ('And', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('my', 'PRP$'),\n",
       " ('stress', 'JJ'),\n",
       " ('busters', 'NNS'),\n",
       " ('too', 'RB')]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent_tokens_tags = nltk.pos_tag(nltk.word_tokenize(re.sub('[^A-Za-z]+',' ',test_sentence)))\n",
    "test_sent_tokens_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos('better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('better',get_wordnet_pos('better'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos('watching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('watching',get_wordnet_pos('watching'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Waaooo', 'such', 'an', 'amazing', 'match', 'I', 'really', 'enjoy', 'watch', 'this', 'event', 'of', 'WWE', 'and', 'pro', 'wrestling', 'And', 'they', 'be', 'my', 'stress', 'buster', 'too']\n"
     ]
    }
   ],
   "source": [
    "print([wnet_lemma.lemmatize(word,get_wordnet_pos(tag)) for word,tag in test_sent_tokens_tags])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Now, the results are quite good. Here, if you closely examine 'amazing' is unchanged because its context in the snetence belongs to the Adjective class and in this category its lemma is 'amazing'. Therefore, it remains the same.**\n",
    "\n",
    "### **Lets check the same with a sentence having some homonyms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "homonym_sent = (\"They refuse to permit us to obtain the refuse permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'us',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homonym_sent_tokens = nltk.word_tokenize(homonym_sent,preserve_line=False)\n",
    "homonym_sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homonym_sent_tokens_tags = nltk.pos_tag(homonym_sent_tokens)\n",
    "homonym_sent_tokens_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refuse'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('refuse','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'refuse', 'to', 'permit', 'u', 'to', 'obtain', 'the', 'refuse', 'permit']\n"
     ]
    }
   ],
   "source": [
    "print([wnet_lemma.lemmatize(word,get_wordnet_pos(tag)) for word,tag in homonym_sent_tokens_tags])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notice that refuse and permit both appear as a present tense verb (VBP) and a noun (NN). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones).**\n",
    "\n",
    "### **Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`TextBlob Lemmatizer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='beauti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = TextBlob(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Waaooo what a beautifull fight I really enjoy watching WWE and pro-wrestling And they are my stress buster too'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \". join([w.lemmatize() for w in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`TreeTaggerWrapper Lemmatizer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper as ttpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ttpw.TreeTagger(TAGLANG='en',TAGDIR=\"C:\\TreeTagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tagger.tag_text(text=test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [t.split('\\t')[-1] for t in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waaooo',\n",
       " ',',\n",
       " 'what',\n",
       " 'a',\n",
       " 'beautifull',\n",
       " 'fight',\n",
       " '.',\n",
       " 'I',\n",
       " 'really',\n",
       " 'enjoy',\n",
       " 'watch',\n",
       " 'WWE',\n",
       " 'and',\n",
       " 'pro-wrestling',\n",
       " '.',\n",
       " 'and',\n",
       " ',',\n",
       " 'they',\n",
       " 'be',\n",
       " 'my',\n",
       " 'stress',\n",
       " 'buster',\n",
       " 'too',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\rajsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2890\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-d5049d8f0b56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Wordnet_Lemmas'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwnet_lemma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rajsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rajsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2891\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2893\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "text_df['Wordnet_Lemmas'] = text_df[''].apply(lambda row: [wnet_lemma.lemmatize(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***`Featurization`***\n",
    "### **1. BAG of WORDS (BOW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(str(preprocess_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW = cv.fit_transform(''.join(preprocess_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(cv.get_feature_names()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_stop_words())       ## Here, in countvectoriser we can also initialize the stopwords but in this case I have kept it blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_features = pd.DataFrame(BOW.toarray(),columns=cv.get_feature_names())\n",
    "bow_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. N-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([' '.join(final_corpus_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = cv2.fit_transform([' '.join(final_corpus_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_features = pd.DataFrame(ngrams.toarray(),columns=cv2.get_feature_names())\n",
    "ngrams_features.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
